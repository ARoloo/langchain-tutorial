# Lektion-2

## Einf√ºhrung
In dieser Lektion werden wir uns mit der Verwendung der `ChatPromptTemplate`-Klasse
aus dem `langchain_core.prompts`-Modul besch√§ftigen. Diese Klasse erm√∂glicht es uns,
Vorlagen f√ºr Prompts zu erstellen, die wir an das LLM-Modell senden k√∂nnen.
Wir werden verschiedene Methoden zur Erstellung von Prompts untersuchen und sehen,
wie wir diese in unserer Anwendung nutzen k√∂nnen. üöÄ‚ú®

### 1. Importieren der Klasse
Zuerst m√ºssen wir die `ChatPromptTemplate`-Klasse importieren:

```python
from langchain_core.prompts import ChatPromptTemplate
```

### 2. Erstellen einer Prompt-Vorlage
Wir k√∂nnen eine Prompt-Vorlage erstellen, indem wir die Methode `from_template` verwenden. Hier ist ein Beispiel, das einen Witz √ºber ein bestimmtes Thema erz√§hlt:

```python
prompt_template = ChatPromptTemplate.from_template("Erz√§hl mir einen Witz √ºber ein {tema}")
```

### 3. Aufruf der Prompt-Vorlage
Um die Vorlage zu verwenden, rufen wir die `invoke`-Methode auf und √ºbergeben die erforderlichen Parameter:

```python
prompt = prompt_template.invoke({"tema": "Kamele"})
```

### 4. Verwendung von Nachrichten-Templates
Wir k√∂nnen auch Vorlagen f√ºr Nachrichten erstellen, um den Kontext der Interaktion zu steuern. Hier ist ein Beispiel:

```python
prompt_template = ChatPromptTemplate.from_messages(
    [
        ("system","Reagiert auf den Nutzer als Pirat, mit Emojis, wenn der Kontext es zul√§sst."),
        ("human", "{input}")
    ]
)
```

### 5. Erstellen einer Kette von Vorlagen
Wir k√∂nnen auch eine Kette von Vorlagen erstellen, um die Interaktion zu steuern:

```python
chain = prompt_template | llm
response = chain.invoke({"tema": "Kamele"})
print(response.content)
```

Viel Spa√ü beim Experimentieren mit der `ChatPromptTemplate`-Klasse! üéâüéà
